import google.generativeai as genai
import cv2
import base64
from PIL import Image
import io
import re
import numpy as np # For np.linspace
from core.config import Config
from core.logger import logger
import os
from datetime import datetime

class VLMAnalyzer:
    def __init__(self, config: Config):
        self.config = config
        genai.configure(api_key=self.config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(self.config.VLM_MODEL_NAME)
        self.generation_config = genai.types.GenerationConfig(
            temperature=0.1,
        )
        logger.success(f"VLM model '{self.config.VLM_MODEL_NAME}' initialized for analysis.")

    def _save_frames(self, frame_sequence: list):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs(self.config.VLM_FRAMES_DIR, exist_ok=True)
        for i, frame in enumerate(frame_sequence):
            frame.save(f"{self.config.VLM_FRAMES_DIR}/frame_{timestamp}_{i}.jpg")
        logger.debug(f"Saved {len(frame_sequence)} frames to {self.config.VLM_FRAMES_DIR} directory")

    def analyze_and_explain(self, frame_sequence: list, constraints: str) -> dict:
        """
        Analyzes a sequence of frames (as a list of PIL Image objects) against given constraints using VLM.
        Args:
            frame_sequence (list): A list of PIL Image objects for the video segment.
            constraints (str): Natural language constraints generated by the LLM.
        Returns:
            dict: Decision (cheating/not cheating) and explanation.
        """
        if not frame_sequence:
            return {"decision": "No Visual Data", "explanation": "No frames provided for analysis."}

        # TODO: Debug save the frames to a output/vlm_frames directory
        self._save_frames(frame_sequence)

        logger.step(f"VLMAnalyzer: Analyzing {len(frame_sequence)} frames with constraints: {constraints}")

        # Prepare content for VLM - Google Gemini's vision models accept up to 16 images
        # Select evenly spaced frames if the sequence is too long
        max_images_for_gemini = 16
        if len(frame_sequence) > max_images_for_gemini:
            indices = np.linspace(0, len(frame_sequence) - 1, max_images_for_gemini, dtype=int)
            selected_frames = [frame_sequence[i] for i in indices]
        else:
            selected_frames = frame_sequence

        image_parts = []
        for img in selected_frames:
            # Ensure img is a PIL Image
            if not isinstance(img, Image.Image):
                logger.warning("Warning: Expected PIL Image, got different type. Attempting conversion.")
                if isinstance(img, np.ndarray):
                    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                else:
                    logger.error(f"Error: Cannot convert image type {type(img)}")
                    continue # Skip this frame

            buffered = io.BytesIO()
            img.save(buffered, format="JPEG", quality=80) # Use JPEG with quality for smaller size
            image_parts.append({
                'mime_type': 'image/jpeg',
                'data': base64.b64encode(buffered.getvalue()).decode('utf-8')
            })

        if not image_parts:
            return {"decision": "No Valid Images", "explanation": "Could not prepare valid images for VLM analysis."}

        prompt_text = f"""You are a specialized AI assistant for proctoring exams. Your task is to analyze a sequence of images and determine if a student's actions violate a given rule.

            **Analysis Task:**
            Based *only* on the visual evidence in the image sequence, verify the following:
            > {constraints}

            **Output Requirements:**
            Your response MUST strictly follow this format, with no extra text or markdown (e.g., no `*`, `-`, or `###`). The explanation must be a single, concise paragraph.

            Decision: [Cheating Confirmed / Not Cheating / Ambiguous]
            Explanation: [A brief summary of your reasoning, linking specific visual evidence to the analysis task.]
        """

        prompt_parts = [
            prompt_text,
            *image_parts,
        ]

        try:
            response = self.model.generate_content(
                prompt_parts,
                generation_config=self.generation_config
            )
            
            # Access the first part of the first candidate's content
            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                response_text = response.candidates[0].content.parts[0].text

                # Use regex to parse decision and explanation reliably
                decision_match = re.search(r"Decision:\s*(.+?)(?:\n|$)", response_text, re.IGNORECASE)
                explanation_match = re.search(r"Explanation:\s*(.+)", response_text, re.IGNORECASE | re.DOTALL)

                decision = decision_match.group(1).strip() if decision_match else "Unclear"
                explanation = explanation_match.group(1).strip() if explanation_match else response_text

                return {"decision": decision, "explanation": explanation}
            else:
                # Handle cases where response might be empty or blocked due to safety settings
                safety_ratings = response.prompt_feedback.safety_ratings if response.prompt_feedback else "N/A"
                logger.warning(f"VLM response did not contain expected content or was blocked. Safety Ratings: {safety_ratings}")
                return {"decision": "Error", "explanation": f"VLM could not process the request or response was empty/blocked. Safety: {safety_ratings}"}
        except Exception as e:
            logger.error(f"Error calling VLM for analysis: {e}")
            return {"decision": "Error", "explanation": f"VLM API error: {e}. Check API key, quota, or input content."}


    def extract_person_features(self, person_image: Image.Image) -> str:
        """
        Extracts visual features of a person from a given image using VLM.
        Args:
            person_image (PIL.Image): Cropped image containing only the person.
        Returns:
            str: A structured description of the person's visual features.
        """
        # Convert np.ndarray (BGR) to PIL.Image (RGB)
        if isinstance(person_image, np.ndarray):
            person_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)
            person_image = Image.fromarray(person_image)

        prompt = (
            "You are an expert AI proctoring assistant. Your task is to create a concise visual description of the student in the image. "
            "This description is critical for identifying students in real-time proctoring alerts. "
            "Stick to visual facts from the image only. Do not invent details. "
            "Respond using this exact format:\n\n"
            "Gender: [Man/Woman]. "
            "Hair: [Color and style, e.g., Short black hair]. "
            "Clothing: [Main color and type, text in clothing e.g., Blue T-shirt]. "
            "Glasses: [What kind of glasses, e.g., Round-framed glasses, Glasses, or None]. "
            "Headwear: [e.g., Headphones, Hat, or None]. "
        )

        # Prepare image for VLM
        buffered = io.BytesIO()
        person_image.save(buffered, format="JPEG", quality=80)
        image_part = {
            'mime_type': 'image/jpeg',
            'data': base64.b64encode(buffered.getvalue()).decode('utf-8')
        }

        prompt_parts = [
            prompt,
            image_part,
        ]

        try:
            response = self.model.generate_content(
                prompt_parts,
                generation_config=self.generation_config
            )
            # Parse response text
            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                response_text = response.candidates[0].content.parts[0].text
                return response_text.strip()
            else:
                logger.warning("VLM response did not contain expected content.")
                return "Cannot extract features."
        except Exception as e:
            logger.error(f"Error calling VLM for feature extraction: {e}")
            return f"API error: {e}"