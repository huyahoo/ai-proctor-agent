import google.generativeai as genai
import cv2
import base64
from PIL import Image
import io
from core.config import Config
import numpy as np
import re

class VLMAnalyzer:
    def __init__(self, config: Config):
        self.config = config
        genai.configure(api_key=self.config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(self.config.VLM_MODEL_NAME)
        print(f"VLM model '{self.config.VLM_MODEL_NAME}' initialized for analysis.")

    def analyze_and_explain(self, frame_sequence: list, constraints: str) -> dict:
        """
        Analyzes a sequence of frames (as a list of base64 images) against given constraints using VLM.
        Args:
            frame_sequence (list): A list of PIL Image objects or base64 strings of frames.
            constraints (str): Natural language constraints generated by the LLM.
        Returns:
            dict: Decision (cheating/not cheating) and explanation.
        """
        if not frame_sequence:
            return {"decision": "No Visual Data", "explanation": "No frames provided for analysis."}

        # Prepare content for VLM - max 16 images for Gemini
        max_images_for_gemini = 16
        if len(frame_sequence) > max_images_for_gemini:
            # Select evenly spaced frames if too many
            indices = np.linspace(0, len(frame_sequence) - 1, max_images_for_gemini, dtype=int)
            selected_frames = [frame_sequence[i] for i in indices]
        else:
            selected_frames = frame_sequence

        # Convert PIL Images to byte data for Gemini API
        image_parts = []
        for img in selected_frames:
            if isinstance(img, Image.Image):
                buffered = io.BytesIO()
                img.save(buffered, format="JPEG") # Use JPEG for smaller size
                image_parts.append({
                    'mime_type': 'image/jpeg',
                    'data': base64.b64encode(buffered.getvalue()).decode('utf-8')
                })
            # Assume if not PIL Image, it's already base64 string
            elif isinstance(img, str):
                 image_parts.append({
                    'mime_type': 'image/jpeg', # Assume base64 is JPEG
                    'data': img
                })

        # Construct the prompt for the VLM
        prompt_parts = [
            f"Analyze the following sequence of images from an exam. The previous analysis suggests a suspicious event. Based on these images, and considering the following verification constraints, determine if cheating is confirmed and explain why:\n\nConstraints: {constraints}\n\n",
            *image_parts, # Unpack image parts here
            "\n\nDecision (e.g., 'Cheating Confirmed' or 'Not Cheating'):\nExplanation:"
        ]

        try:
            response = self.model.generate_content(prompt_parts)
            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                response_text = response.candidates[0].content.parts[0].text

                # Parse decision and explanation (simple regex or string splitting)
                decision_match = re.search(r"Decision:\s*(.+?)\n", response_text, re.IGNORECASE)
                explanation_match = re.search(r"Explanation:\s*(.+)", response_text, re.IGNORECASE | re.DOTALL)

                decision = decision_match.group(1).strip() if decision_match else "Unclear"
                explanation = explanation_match.group(1).strip() if explanation_match else response_text

                return {"decision": decision, "explanation": explanation}
            else:
                print("VLM response did not contain expected content.")
                return {"decision": "Error", "explanation": "VLM could not process the request."}
        except Exception as e:
            print(f"Error calling VLM for analysis: {e}")
            return {"decision": "Error", "explanation": f"VLM API error: {e}"}


